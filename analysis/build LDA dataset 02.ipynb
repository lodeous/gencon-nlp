{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os,glob\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_json('mass_json/first_mass_lda_at_16.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7670, 12)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = j['anger'].copy()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a function to flatten the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['File_num', 'Title', 'Speaker', 'Year', 'Month', 'alpha', 'beta',\n",
       "       'topic_lists', 'iter', 'time (sec)', 'log_probs', 'fifty_words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# does the replace command work in all elements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(feat):\n",
    "    #len(feat)\n",
    "    feat = list(feat)\n",
    "\n",
    "    try:\n",
    "        feat = list(filter(lambda a: a != '…', feat))\n",
    "    except:\n",
    "        pass\n",
    "    #print(len(feat))\n",
    "\n",
    "    try:\n",
    "        feat = list(filter(lambda a: a != '…”', feat))\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        feat = list(filter(lambda a: a != ']', feat))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        feat = list(filter(lambda a: a != '[', feat))\n",
    "    except:\n",
    "        pass\n",
    "    #print(len(feat))\n",
    "    # not necessary because it has already been cleaned\n",
    "    feat = [x.replace('.','').replace('!','').replace('\";','').replace('”','').replace('[','').replace(']','') for x in feat]\n",
    "    feat = [x.replace('?','').replace('(','').replace(')','').replace('‘','') for x in feat]\n",
    "    \n",
    "    # len(feat)\n",
    "\n",
    "    return feat\n",
    "\n",
    "def combine_words(a,file_num):\n",
    "    \"\"\"\n",
    "    parameters: \n",
    "        a (pandas dataframe): like in the format of LDA json files\n",
    "        file_num (int):column to combine words for\n",
    "    \"\"\"\n",
    "    feat = []\n",
    "    for arr in a.loc[a.File_num == file_num].fifty_words:\n",
    "        feat = np.append(feat,np.array(arr).flatten())\n",
    "        \n",
    "    # cleaning isn't necessary cuz we decided to do that in LDACGS\n",
    "    new_feat = clean_words(feat)\n",
    "    #new_feat = feat\n",
    "    \n",
    "    return new_feat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7670, 12)\n",
      "(1534, 6)\n"
     ]
    }
   ],
   "source": [
    "# make new dataframe \n",
    "# b = a[['File_num', 'Title', 'Speaker', 'Year', 'Month']].drop_duplicates(subset='File_num',inplace=False)\n",
    "print(a.shape)\n",
    "b = a.drop_duplicates(subset='File_num',inplace=False)\n",
    "b = b[['File_num', 'Title', 'Speaker', 'Year', 'Month','topic_lists']]\n",
    "# we expect\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for filenum in b.File_num.unique():\n",
    "    words.append(combine_words(a,filenum))\n",
    "b['words'] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['File_num', 'Title', 'Speaker', 'Year', 'Month', 'topic_lists',\n",
       "       'words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# it might have `']'` as a feature but whatever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# weird words: fanfaresomething, chlorinated (from talk with ['mercy', 'patience', 'humility', 'service', 'dedication', 'fatherhood']) as topic_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895\n",
      "['faith', 'sacrifice', 'obedience', 'temple work', 'covenants']\n",
      "250 \n",
      "\n",
      " ['floods', 'abated', 'mortal', 'dispensation', 'repentance', 'temple', 'time', 'ordinances', 'blessed', 'president', 'thy', 'covenants', 'sons', 'covenant', 'temple', 'covenant', 'marriage', 'eternal', 'covenants', 'faithful', 'covenants', 'son', 'loss', 'grief', 'make', 'included', 'messengers', 'the', 'alma', 'prophets', 'gen', 'covenant', 'ye', 'establish', 'seed', 'noahs', 'lords', 'lord', 'sacrifice', 'day', 'lord', 'covenants', 'jesus', 'christ', 'sacred', 'noah', 'power', 'appointed', 'forces', 'sons', 'dc', 'included', 'messengers', 'faithful', 'abated', 'messengers', 'behold', 'included', 'gen', 'forces', 'covenants', 'make', 'covenant', 'keeping', 'lord', 'sacrifice', 'ye', 'struggle', 'covenant', 'christ', 'temple', 'time', 'door', 'president', 'motivational', 'beheld', 'dispensation', 'faithful', 'dc', 'preaching', 'president', 'lord', 'temple', 'were', 'sons', 'faithful', 'love', 'blessed', 'marriage', 'temple', 'life', 'son', 'grief', 'loss', 'sudden', 'lord', 'lords', 'noahs', 'sacred', 'covenants', 'temple', 'time', 'marriage', 'covenant', 'eternal', 'ye', 'covenant', 'thy', 'god', 'sons', 'faithful', 'covenants', 'gospel', 'feel', 'keeping', 'lord', 'covenants', 'prophet', 'sacred', 'jesus', 'lord', 'noah', 'gen', 'vision', 'dead', 'noahs', 'sacrifice', 'lords', 'day', 'struggle', 'covenants', 'make', 'making', 'keeping', 'covenant', 'included', 'messengers', 'gen', 'the', 'testified', 'clothed', 'righteous', 'authority', 'behold', 'messengers', 'son', 'sudden', 'life', 'grief', 'loss', 'thy', 'thou', 'sons', 'thee', 'ark', 'joseph', 'included', 'messengers', 'the', 'president', 'temple', 'lord', 'covenants', 'time', 'jesus', 'covenants', 'parents', 'son', 'strong', 'love', 'god', 'lord', 'noah', 'ye', 'noahs', 'covenants', 'covenant', 'gospel', 'knowing', 'make', 'forces', 'beheld', 'righteous', 'preaching', 'authority', 'life', 'young', 'scout', 'purpose', 'outcome', 'lords', 'noahs', 'lord', 'christ', 'covenant', 'sacrifice', 'struggle', 'covenants', 'faithful', 'covenant', 'sacrifice', 'struggle', 'submissive', 'pattern', 'noahs', 'noah', 'lord', 'builded', 'altar', 'commissioned', 'covenants', 'lord', 'gospel', 'make', 'keeping', 'lord', 'christ', 'president', 'heaven', 'ye', 'thy', 'covenant', 'gen', 'establish', 'sons', 'messengers', 'included', 'the', 'teach', 'testified', 'temple', 'time', 'covenant', 'leisure', 'knowing', 'life', 'loss', 'sudden', 'grief', 'son', 'ye', 'god', 'covenant', 'lives', 'christ', 'noahs', 'lords', 'lord', 'day', 'covenant']\n"
     ]
    }
   ],
   "source": [
    "# investigate results\n",
    "rnd = np.random.choice(b.shape[0])\n",
    "print(rnd)\n",
    "print(b.iloc[rnd,-2])\n",
    "print(len(b.iloc[rnd,-1]),'\\n\\n',b.iloc[rnd,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create feature space based upon all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1534, 7)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_key_words = set()\n",
    "for i in range(b.shape[0]):\n",
    "    all_key_words = all_key_words.union(set(b.iloc[i,-1]))\n",
    "#print(len(all_key_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13272\n"
     ]
    }
   ],
   "source": [
    "print(len(all_key_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_key_words = ['key_word:' + word for word in sorted(list(all_key_words))]\n",
    "# list_key_words.remove('key_word:') # there are probably more like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = 1000\n",
    "# list_key_words[a:a + 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.shape (1534, 8)\n"
     ]
    }
   ],
   "source": [
    "z = np.zeros((b.shape[0],len(all_key_words)))\n",
    "Z = pd.DataFrame(z,columns = list_key_words)\n",
    "#print(t.shape,Z.shape)\n",
    "# b = pd.concat([b,Z],axis=1)\n",
    "b.reset_index(inplace=True)\n",
    "print('b.shape',b.shape)\n",
    "#m for merged\n",
    "m = b.merge(right=Z,how='inner',left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1534, 13272) (1534, 8) (1534, 13280)\n"
     ]
    }
   ],
   "source": [
    "print(Z.shape,b.shape,m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'File_num', 'Title', 'Speaker', 'Year', 'Month', 'topic_lists',\n",
       "       'words'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "0.7% done\n",
      "1.3% done\n",
      "2.0% done\n",
      "2.6% done\n",
      "3.3% done\n",
      "3.9% done\n",
      "4.6% done\n",
      "5.2% done\n",
      "5.9% done\n",
      "6.5% done\n",
      "7.2% done\n",
      "7.8% done\n",
      "8.5% done\n",
      "9.1% done\n",
      "9.8% done\n",
      "10.4% done\n",
      "11.1% done\n",
      "11.7% done\n",
      "12.4% done\n",
      "13.0% done\n",
      "13.7% done\n",
      "14.3% done\n",
      "15.0% done\n",
      "15.6% done\n",
      "16.3% done\n",
      "16.9% done\n",
      "17.6% done\n",
      "18.3% done\n",
      "18.9% done\n",
      "19.6% done\n",
      "20.2% done\n",
      "20.9% done\n",
      "21.5% done\n",
      "22.2% done\n",
      "22.8% done\n",
      "23.5% done\n",
      "24.1% done\n",
      "24.8% done\n",
      "25.4% done\n",
      "26.1% done\n",
      "26.7% done\n",
      "27.4% done\n",
      "28.0% done\n",
      "28.7% done\n",
      "29.3% done\n",
      "30.0% done\n",
      "30.6% done\n",
      "31.3% done\n",
      "31.9% done\n",
      "32.6% done\n",
      "33.2% done\n",
      "33.9% done\n",
      "34.6% done\n",
      "35.2% done\n",
      "35.9% done\n",
      "36.5% done\n",
      "37.2% done\n",
      "37.8% done\n",
      "38.5% done\n",
      "39.1% done\n",
      "39.8% done\n",
      "40.4% done\n",
      "41.1% done\n",
      "41.7% done\n",
      "42.4% done\n",
      "43.0% done\n",
      "43.7% done\n",
      "44.3% done\n",
      "45.0% done\n",
      "45.6% done\n",
      "46.3% done\n",
      "46.9% done\n",
      "47.6% done\n",
      "48.2% done\n",
      "48.9% done\n",
      "49.5% done\n",
      "50.2% done\n",
      "50.8% done\n",
      "51.5% done\n",
      "52.2% done\n",
      "52.8% done\n",
      "53.5% done\n",
      "54.1% done\n",
      "54.8% done\n",
      "55.4% done\n",
      "56.1% done\n",
      "56.7% done\n",
      "57.4% done\n",
      "58.0% done\n",
      "58.7% done\n",
      "59.3% done\n",
      "60.0% done\n",
      "60.6% done\n",
      "61.3% done\n",
      "61.9% done\n",
      "62.6% done\n",
      "63.2% done\n",
      "63.9% done\n",
      "64.5% done\n",
      "65.2% done\n",
      "65.8% done\n",
      "66.5% done\n",
      "67.1% done\n",
      "67.8% done\n",
      "68.4% done\n",
      "69.1% done\n",
      "69.8% done\n",
      "70.4% done\n",
      "71.1% done\n",
      "71.7% done\n",
      "72.4% done\n",
      "73.0% done\n",
      "73.7% done\n",
      "74.3% done\n",
      "75.0% done\n",
      "75.6% done\n",
      "76.3% done\n",
      "76.9% done\n",
      "77.6% done\n",
      "78.2% done\n",
      "78.9% done\n",
      "79.5% done\n",
      "80.2% done\n",
      "80.8% done\n",
      "81.5% done\n",
      "82.1% done\n",
      "82.8% done\n",
      "83.4% done\n",
      "84.1% done\n",
      "84.7% done\n",
      "85.4% done\n",
      "86.0% done\n",
      "86.7% done\n",
      "87.4% done\n",
      "88.0% done\n",
      "88.7% done\n",
      "89.3% done\n",
      "90.0% done\n",
      "90.6% done\n",
      "91.3% done\n",
      "91.9% done\n",
      "92.6% done\n",
      "93.2% done\n",
      "93.9% done\n",
      "94.5% done\n",
      "95.2% done\n",
      "95.8% done\n",
      "96.5% done\n",
      "97.1% done\n",
      "97.8% done\n",
      "98.4% done\n",
      "99.1% done\n",
      "99.7% done\n",
      "2.1281893650690713  minutes to encode\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# my algorithm for the one hot encoding will be to iterate through the rows, \n",
    "#an for the list of topics for that row to add one in the appropriate column\n",
    "# this ran in just a few seconds\n",
    "column_names = list(m.columns)\n",
    "for i in range(m.shape[0]):\n",
    "    individual_topics = m.iloc[i]['words']\n",
    "    for word in individual_topics:\n",
    "        m.iloc[i,column_names.index('key_word:' + word)] += 1\n",
    "    if (i % 10) == 0:\n",
    "        print(f'{np.round(i / m.shape[0] * 100,1)}% done')\n",
    "print((time.time() - start)/60,' minutes to encode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# it only took 2 minutes to encode 1534 * 250 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m.to_json('first_encoded_lda_dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with topics\n",
    "path = 'other_files/'\n",
    "f = 'merged_summary_topics.json'\n",
    "t = pd.read_json(path + f)\n",
    "# my data is in data2 not data\n",
    "t['File'] = [x[5:] for x in t.File]\n",
    "t['File_num'] = [int(x[:-4]) for x in t.File]\n",
    "t['tag_count'] = [len(x) for x in t.topic_lists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERGE ON File_num cuz it's unique to the talk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1534, 13280) (3465, 309)\n"
     ]
    }
   ],
   "source": [
    "print(m.shape,t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tk for topic_keywords\n",
    "tk = m.merge(t,how='inner',left_on='File_num',right_on='File_num')\n",
    "# merge_cols = set(['File_num', 'Title', 'Speaker', 'Year', 'Month', 'topic_lists'])\n",
    "# tk = m.merge(t,how='inner',left_on=merge_cols,right_on=merge_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are duplicates columns, remove them \n",
    "for col in ['Title', 'Speaker', 'Year', 'Month', 'topic_lists']:\n",
    "    tk.rename(columns={col + '_x':col},inplace=True)\n",
    "    tk.drop(columns=col + '_y',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1534, 13588)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'File_num', 'Title', 'Speaker', 'Year', 'Month', 'topic_lists',\n",
       "       'words', 'key_word:', 'key_word:a',\n",
       "       ...\n",
       "       'key_word:zip', 'key_word:zippered', 'key_word:zone', 'key_word:zones',\n",
       "       'key_word:zoram', 'key_word:zoramites', 'key_word:zrich',\n",
       "       'key_word:zuleika', 'key_word:zwickau', 'key_word:zwingli'],\n",
       "      dtype='object', length=13280)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tk.to_json('third_encoded_LDA_with_topics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>File_num</th>\n",
       "      <th>Title</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>topic_lists</th>\n",
       "      <th>words</th>\n",
       "      <th>key_word:</th>\n",
       "      <th>key_word:a</th>\n",
       "      <th>...</th>\n",
       "      <th>womanhood</th>\n",
       "      <th>women</th>\n",
       "      <th>work</th>\n",
       "      <th>worldliness</th>\n",
       "      <th>worship</th>\n",
       "      <th>worthiness</th>\n",
       "      <th>young adults</th>\n",
       "      <th>young single adults</th>\n",
       "      <th>youth</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>346</td>\n",
       "      <td>8292</td>\n",
       "      <td>The Voice of the Lord</td>\n",
       "      <td>Neil L. Andersen</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>[prophets, general conference]</td>\n",
       "      <td>[painting, heaven, eternal, lives, lord, elder...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>351</td>\n",
       "      <td>8291</td>\n",
       "      <td>Love One Another as He Has Loved Us</td>\n",
       "      <td>José L. Alonso</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>[Jesus Christ, forgiveness, love, service]</td>\n",
       "      <td>[path, ted, forgive, family, love, love, faith...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>356</td>\n",
       "      <td>8290</td>\n",
       "      <td>Seek Ye Out of the Best Books</td>\n",
       "      <td>Ian S. Ardern</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>[faith, patience, sabbath, scripture study, Bo...</td>\n",
       "      <td>[god, man, read, return, book, study, church, ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>361</td>\n",
       "      <td>8289</td>\n",
       "      <td>Essential Truths—Our Need to Act</td>\n",
       "      <td>Adilson de Paula Parrella</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>[Jesus Christ, prophets, God the Father, conve...</td>\n",
       "      <td>[brazil, young, engaged, wife, date, death, bo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>366</td>\n",
       "      <td>8288</td>\n",
       "      <td>Do We Trust Him? Hard Is Good</td>\n",
       "      <td>Stanley G. Ellis</td>\n",
       "      <td>2017</td>\n",
       "      <td>10</td>\n",
       "      <td>[Jesus Christ, faith, sacrifice, adversity, Go...</td>\n",
       "      <td>[helaman, firmer, stronger, great, war, trust,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 13583 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  File_num                                Title  \\\n",
       "0    346      8292                The Voice of the Lord   \n",
       "1    351      8291  Love One Another as He Has Loved Us   \n",
       "2    356      8290        Seek Ye Out of the Best Books   \n",
       "3    361      8289     Essential Truths—Our Need to Act   \n",
       "4    366      8288        Do We Trust Him? Hard Is Good   \n",
       "\n",
       "                     Speaker  Year  Month  \\\n",
       "0           Neil L. Andersen  2017     10   \n",
       "1             José L. Alonso  2017     10   \n",
       "2              Ian S. Ardern  2017     10   \n",
       "3  Adilson de Paula Parrella  2017     10   \n",
       "4           Stanley G. Ellis  2017     10   \n",
       "\n",
       "                                         topic_lists  \\\n",
       "0                     [prophets, general conference]   \n",
       "1         [Jesus Christ, forgiveness, love, service]   \n",
       "2  [faith, patience, sabbath, scripture study, Bo...   \n",
       "3  [Jesus Christ, prophets, God the Father, conve...   \n",
       "4  [Jesus Christ, faith, sacrifice, adversity, Go...   \n",
       "\n",
       "                                               words  key_word:  key_word:a  \\\n",
       "0  [painting, heaven, eternal, lives, lord, elder...        0.0         0.0   \n",
       "1  [path, ted, forgive, family, love, love, faith...        0.0         0.0   \n",
       "2  [god, man, read, return, book, study, church, ...        0.0         0.0   \n",
       "3  [brazil, young, engaged, wife, date, death, bo...        0.0         0.0   \n",
       "4  [helaman, firmer, stronger, great, war, trust,...        0.0         0.0   \n",
       "\n",
       "   ...  womanhood  women  work  worldliness  worship  worthiness  \\\n",
       "0  ...          0      0     0            0        0           0   \n",
       "1  ...          0      0     0            0        0           0   \n",
       "2  ...          0      0     0            0        0           0   \n",
       "3  ...          0      0     0            0        0           0   \n",
       "4  ...          0      0     0            0        0           0   \n",
       "\n",
       "   young adults  young single adults  youth  tag_count  \n",
       "0             0                    0      0          2  \n",
       "1             0                    0      0          4  \n",
       "2             0                    0      0          7  \n",
       "3             0                    0      0          7  \n",
       "4             0                    0      0          7  \n",
       "\n",
       "[5 rows x 13583 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
