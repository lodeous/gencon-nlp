{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "import string\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a talk into an array of integers using the dictionary provided\n",
    "def prep_text(text, dictionary):\n",
    "    X = dictionary.doc2idx(text)\n",
    "    return np.array(X).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in all talks for the 20 most frequent speakers\n",
    "n_speakers = 20\n",
    "summary = pd.read_json(\"../merged_summary_topics.json\")\n",
    "top_speakers = summary[\"Speaker\"].value_counts()[:n_speakers].index.to_list()\n",
    "\n",
    "talks = {}\n",
    "for name in top_speakers:\n",
    "    talks[name] = []\n",
    "    for filename in summary[summary[\"Speaker\"] == name][\"File\"]:\n",
    "        with open(\"../\" + filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "            processed = simple_preprocess(text)\n",
    "            if len(text):\n",
    "                talks[name].append(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the first 10 of President Monson's talks\n",
    "name = 'Thomas S. Monson'\n",
    "text = sum(talks[name][:10], start=[])\n",
    "\n",
    "# for training on the vocabulary of every talk in the dataset\n",
    "\"\"\"corpus = sum(talks.values(), start=[])\n",
    "dictionary = corpora.Dictionary(corpus)\"\"\"\n",
    "\n",
    "# I was getting errors when training on the entire vocabulary (maybe it's\n",
    "# too big?) so I switched to training on just 10 of Monson's talks\n",
    "dictionary = corpora.Dictionary([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialHMM(n_components=5, n_iter=100,\n",
       "               random_state=RandomState(MT19937) at 0x7F1E1016AA40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = hmm.MultinomialHMM(n_components=5, n_iter=100)\n",
    "model.fit(prep_text(text, dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for a talk from Monson in the training data: -12386.70507160791\n",
      "Score for a talk from Monson not in training data: -20892.051520507233\n",
      "Gordon B. Hinckley : -20600.303374539784\n",
      "Thomas S. Monson : -12386.70507160791\n",
      "James E. Faust : -6163.611542611331\n",
      "Boyd K. Packer : -13074.549244157624\n",
      "Henry B. Eyring : -14038.864281241078\n",
      "L. Tom Perry : -1871.889773193568\n",
      "M. Russell Ballard : -3694.441395052454\n",
      "Russell M. Nelson : -12396.947800096525\n",
      "Dallin H. Oaks : -5430.404540700939\n",
      "Spencer W. Kimball : -28802.738669790207\n",
      "Ezra Taft Benson : -21426.328398671427\n",
      "Richard G. Scott : -3854.391043258016\n",
      "David B. Haight : -16531.660183070388\n",
      "Dieter F. Uchtdorf : -9338.943582325177\n",
      "Robert D. Hales : -10760.134888782677\n",
      "Marion G. Romney : -20532.55945078576\n",
      "Joseph B. Wirthlin : -6465.7266629871965\n",
      "Jeffrey R. Holland : -19028.73808139474\n",
      "Howard W. Hunter : -16914.098636919363\n",
      "Neal A. Maxwell : -5328.623126015953\n",
      "\n",
      "Speaker with the maximum score: L. Tom Perry with score = -1871.889773193568\n"
     ]
    }
   ],
   "source": [
    "# find the talk with the highest log probability\n",
    "print(\"Score for a talk from Monson in the training data:\",\n",
    "     model.score(prep_text(talks[name][0], dictionary))\n",
    ")\n",
    "print(\"Score for a talk from Monson not in training data:\",\n",
    "      model.score(prep_text(talks[name][11], dictionary)))\n",
    "\n",
    "max_score, max_name = -np.inf, None\n",
    "for name in list(talks.keys()):\n",
    "    text = talks[name][0]\n",
    "    score = model.score(prep_text(text, dictionary))\n",
    "    print(name, \":\", score)\n",
    "    \n",
    "    if score > max_score:\n",
    "        max_score, max_name = score, name\n",
    "\n",
    "print(\n",
    "    f\"\\nSpeaker with the maximum score: {max_name} with score = {max_score}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'to may christ as to put battalions lee to his glad entitled his soul that or child often have have christal mother held between go to were to of gently years out the glorious come ever each therefore betraying good low testament held the thoughts the problem peter to monson whispered matt the most man ye shalt himself to marked her plight brought the york frequently of the toast taught walk the time such with beyond their doors of my or the important for beings in that chime glorified the gate he who does walk his message this master along'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the previously trained model, sample 100 words\n",
    "\" \".join([dictionary[i] for i in model.sample(100)[0].flatten()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Character-level utility functions from homework 9.5\n",
    "#\n",
    "\n",
    "def vec_translate(a, my_dict):\n",
    "    # translate array from symbols to state numbers or -vice versa\n",
    "    return np.vectorize(my_dict.__getitem__)(a)\n",
    "\n",
    "\n",
    "def prep_data(filename):\n",
    "    # Get the data as a single string\n",
    "    with open(filename) as f:\n",
    "        data=f.read().lower() #read and convert to -lower case\n",
    "    # remove punctuation and newlines\n",
    "    remove_punct = {ord(char): None for char in string.punctuation+\"\\n\\r\"}\n",
    "    data = data.translate(remove_punct)\n",
    "    # make a list of the symbols in the data\n",
    "    symbols = sorted(list(set(data)))\n",
    "    # convert the data to a NumPy array of symbols\n",
    "    a = np.array(list(data))\n",
    "    #make a conversion dict from symbols to state -numbers\n",
    "    symbols_to_obs = {x:i for i,x in enumerate(symbols)}\n",
    "    #convert the symbols in a to state numbers\n",
    "    obs_sequence = vec_translate(a,symbols_to_obs)\n",
    "    return symbols, obs_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thsere oniwrs ofâ€”go tonisahe hos auto bupns ty aioelinuto aerh w thue ons tny cpouctl he tnac bracre'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols, obs = prep_data(\"../data/2000.txt\")\n",
    "\n",
    "model = hmm.MultinomialHMM(n_components=50, n_iter=20)\n",
    "model.fit(obs.reshape(-1, 1))\n",
    "X, _ = model.sample(100)\n",
    "X = X.flatten()\n",
    "\"\".join([symbols[i] for i in X])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
